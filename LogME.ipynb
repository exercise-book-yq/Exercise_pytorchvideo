{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LogME.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNhGBAHmufLTHjL00Ko7Sjv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zhengyouqiang/Exercise_pytorchvideo/blob/main/LogME.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kX7SRR6dslFq"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qVbDxEG_spRF"
      },
      "source": [
        "# Abstract\n",
        " The old pilots cannot handle emerging un-supervised pre-trained models or regression tasks.\n",
        " *The Logarithm of Maximum Evidence (LogME)* estimates the maximum value of label\n",
        "evidence given features extracted by pre-trained\n",
        "models. \n",
        "LogME is fast, accurate, and general, character-\n",
        "izing itself as the first practical method for as-\n",
        "sessing pre-trained models. \n",
        "\n",
        "# Introduction\n",
        "The transfer learn-\n",
        "ing paradigm *“pre-training→fine-tuning”* enjoys tremen-\n",
        "dous success in both vision (Kornblith et al.,2019) and\n",
        "language (Devlin et al.,2019) communities, and continues\n",
        "to expand to communities like geometric learning (Hu et al.,\n",
        "2020).\n",
        "\n",
        "# Problem\n",
        "The problem is non-trivial and task adaptive, considering\n",
        "that *different tasks favor different pre-trained models*. \n",
        "\n",
        "# Comparsion\n",
        "LEEP being fast, prior methods are not accurate and are\n",
        "specialized for transferring supervised pre-trained models\n",
        "to classification. They cannot apply to either contrastive\n",
        "pre-trained models (He et al.,2020;Chen et al.,2020a),\n",
        "unsupervised pre-trained language models (Devlin et al.,\n",
        "2019;Liu et al.,2019), or regression tasks.\n",
        "\n",
        "LOGME : The maximum value of label evidence\n",
        "(marginalized likelihood) given extracted features is calcu-\n",
        "lated, providing a general probabilistic approach that is ap-\n",
        "plicable to both classification and regression tasks.\n",
        "\n",
        "# Related Works\n",
        "\n",
        "### Transfer learning\n",
        " a broad research area containing transductive transfer, inductive transfer, task\n",
        "transfer learning, and so on. \n",
        "\n",
        "com-monly known as domain adaptation\n",
        "with the focus on eliminating domain shifts between two do-\n",
        "mains.\n",
        "\n",
        "transfer learning usually refers to inductive transfer, the topic we are\n",
        "concerned about in this paper.\n",
        "\n",
        "# Experiment\n",
        "we can use simple metrics like top-1 accuracy\n",
        "or top-k accuracy (whether top-k in{Sm}Mm=1are also top-\n",
        "k in{Tm}Mm=1). \n"
      ]
    }
  ]
}