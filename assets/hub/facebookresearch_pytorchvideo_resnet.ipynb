{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "colab": {
      "name": "facebookresearch_pytorchvideo_resnet.ipynb",
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "27c7e5ddf448427a927008f67d1c909e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_61093455bfdc44edb920f2444885171c",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_f41565e13e8748a68b05c1c909b69dcb",
              "IPY_MODEL_87a5447f2dc64251880b830b86b3482f",
              "IPY_MODEL_acf226e0ee5348d087b800da905bf288"
            ]
          }
        },
        "61093455bfdc44edb920f2444885171c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f41565e13e8748a68b05c1c909b69dcb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_380c94db3e3244179882ee5644e2f3c8",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a5abc2b3fe0d48f19ccf239c8c9bb2af"
          }
        },
        "87a5447f2dc64251880b830b86b3482f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_6ba853b7aa3b43139ed350e1daa19ff5",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 260018489,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 260018489,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_29b37eaeb0e44e909c47305a2769b7a5"
          }
        },
        "acf226e0ee5348d087b800da905bf288": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_c6cfb7c3dbbd46fea272e4cabcab5b73",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 248M/248M [00:21&lt;00:00, 13.1MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_9ad6c8bbc02f40278a2129b87c930987"
          }
        },
        "380c94db3e3244179882ee5644e2f3c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a5abc2b3fe0d48f19ccf239c8c9bb2af": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6ba853b7aa3b43139ed350e1daa19ff5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "29b37eaeb0e44e909c47305a2769b7a5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c6cfb7c3dbbd46fea272e4cabcab5b73": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "9ad6c8bbc02f40278a2129b87c930987": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "downtown-spell"
      },
      "source": [
        "# 3D ResNet\n",
        "\n",
        "*Author: FAIR PyTorchVideo*\n",
        "\n",
        "**Resnet Style Video classification networks pretrained on the Kinetics 400 dataset**\n",
        "\n",
        "\n",
        "### Example Usage\n",
        "\n",
        "#### Imports\n",
        "\n",
        "Load the model:"
      ],
      "id": "downtown-spell"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W0AmViLUAxvA",
        "outputId": "994ca1af-ca02-48e1-a5aa-5e69dde53884",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install pytorchvideo"
      ],
      "id": "W0AmViLUAxvA",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytorchvideo\n",
            "  Downloading pytorchvideo-0.1.3.tar.gz (128 kB)\n",
            "\u001b[K     |████████████████████████████████| 128 kB 4.0 MB/s \n",
            "\u001b[?25hCollecting fvcore\n",
            "  Downloading fvcore-0.1.5.post20211023.tar.gz (49 kB)\n",
            "\u001b[K     |████████████████████████████████| 49 kB 5.2 MB/s \n",
            "\u001b[?25hCollecting av\n",
            "  Downloading av-8.0.3-cp37-cp37m-manylinux2010_x86_64.whl (37.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 37.2 MB 31 kB/s \n",
            "\u001b[?25hCollecting parameterized\n",
            "  Downloading parameterized-0.8.1-py2.py3-none-any.whl (26 kB)\n",
            "Collecting iopath\n",
            "  Downloading iopath-0.1.9-py3-none-any.whl (27 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from fvcore->pytorchvideo) (1.19.5)\n",
            "Collecting yacs>=0.1.6\n",
            "  Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 48.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from fvcore->pytorchvideo) (4.62.3)\n",
            "Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.7/dist-packages (from fvcore->pytorchvideo) (1.1.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from fvcore->pytorchvideo) (7.1.2)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from fvcore->pytorchvideo) (0.8.9)\n",
            "Collecting portalocker\n",
            "  Downloading portalocker-2.3.2-py2.py3-none-any.whl (15 kB)\n",
            "Building wheels for collected packages: pytorchvideo, fvcore\n",
            "  Building wheel for pytorchvideo (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pytorchvideo: filename=pytorchvideo-0.1.3-py3-none-any.whl size=183829 sha256=d1a5a5d1b4db27b5230c76723abdea2c338630d6854820dd6d140eb5ba7781a7\n",
            "  Stored in directory: /root/.cache/pip/wheels/d4/a7/4c/bada8b1065ae9befac2da6d7f6648cd6718681eb7901ca226d\n",
            "  Building wheel for fvcore (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fvcore: filename=fvcore-0.1.5.post20211023-py3-none-any.whl size=60947 sha256=d493b99d645d22acee7f89dfad2ff358a40fd4258d1fc4a0c58d384ffc4db47f\n",
            "  Stored in directory: /root/.cache/pip/wheels/16/98/fc/252d62cab6263c719120e06b28f3378af59b52ce7a20e81852\n",
            "Successfully built pytorchvideo fvcore\n",
            "Installing collected packages: pyyaml, portalocker, yacs, iopath, parameterized, fvcore, av, pytorchvideo\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed av-8.0.3 fvcore-0.1.5.post20211023 iopath-0.1.9 parameterized-0.8.1 portalocker-2.3.2 pytorchvideo-0.1.3 pyyaml-6.0 yacs-0.1.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "finnish-drill",
        "outputId": "e08d3a15-40cc-46ed-85df-303bc63e347a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67,
          "referenced_widgets": [
            "27c7e5ddf448427a927008f67d1c909e",
            "61093455bfdc44edb920f2444885171c",
            "f41565e13e8748a68b05c1c909b69dcb",
            "87a5447f2dc64251880b830b86b3482f",
            "acf226e0ee5348d087b800da905bf288",
            "380c94db3e3244179882ee5644e2f3c8",
            "a5abc2b3fe0d48f19ccf239c8c9bb2af",
            "6ba853b7aa3b43139ed350e1daa19ff5",
            "29b37eaeb0e44e909c47305a2769b7a5",
            "c6cfb7c3dbbd46fea272e4cabcab5b73",
            "9ad6c8bbc02f40278a2129b87c930987"
          ]
        }
      },
      "source": [
        "import torch\n",
        "# Choose the `slow_r50` model\n",
        "from pytorchvideo.models.hub import (  # noqa: F401, E402\n",
        "    c2d_r50,\n",
        "    csn_r101,\n",
        "    efficient_x3d_s,\n",
        "    efficient_x3d_xs,\n",
        "    i3d_r50,\n",
        "    mvit_base_16,\n",
        "    mvit_base_16x4,\n",
        "    mvit_base_32x3,\n",
        "    r2plus1d_r50,\n",
        "    slow_r50,\n",
        "    slow_r50_detection,\n",
        "    slowfast_16x8_r101_50_50,\n",
        "    slowfast_r50,\n",
        "    slowfast_r50_detection,\n",
        "    slowfast_r101,\n",
        "    x3d_l,\n",
        "    x3d_m,\n",
        "    x3d_s,\n",
        "    x3d_xs,\n",
        ") \n",
        "model=slow_r50(pretrained=True)"
      ],
      "id": "finnish-drill",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://dl.fbaipublicfiles.com/pytorchvideo/model_zoo/kinetics/SLOW_8x8_R50.pyth\" to /root/.cache/torch/hub/checkpoints/SLOW_8x8_R50.pyth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "27c7e5ddf448427a927008f67d1c909e",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0.00/248M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "valued-newfoundland"
      },
      "source": [
        "Import remaining functions:"
      ],
      "id": "valued-newfoundland"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "elect-martin",
        "outputId": "11e8c28e-2d58-4623-95ca-723a1b746f17",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import json\n",
        "import urllib\n",
        "from pytorchvideo.data.encoded_video import EncodedVideo\n",
        "\n",
        "from torchvision.transforms import Compose, Lambda\n",
        "from torchvision.transforms._transforms_video import (\n",
        "    CenterCropVideo,\n",
        "    NormalizeVideo,\n",
        ")\n",
        "from pytorchvideo.transforms import (\n",
        "    ApplyTransformToKey,\n",
        "    ShortSideScale,\n",
        "    UniformTemporalSubsample\n",
        ")"
      ],
      "id": "elect-martin",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/_functional_video.py:6: UserWarning: The _functional_video module is deprecated. Please use the functional module instead.\n",
            "  \"The _functional_video module is deprecated. Please use the functional module instead.\"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/_transforms_video.py:26: UserWarning: The _transforms_video module is deprecated. Please use the transforms module instead.\n",
            "  \"The _transforms_video module is deprecated. Please use the transforms module instead.\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "controlled-compression"
      },
      "source": [
        "#### Setup\n",
        "\n",
        "Set the model to eval mode and move to desired device."
      ],
      "id": "controlled-compression"
    },
    {
      "cell_type": "code",
      "metadata": {
        "attributes": {
          "classes": [
            "python "
          ],
          "id": ""
        },
        "id": "unusual-elephant"
      },
      "source": [
        "# Set to GPU or CPU\n",
        "device = \"cpu\"\n",
        "model = model.eval()\n",
        "model = model.to(device)"
      ],
      "id": "unusual-elephant",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dense-cincinnati"
      },
      "source": [
        "Download the id to label mapping for the Kinetics 400 dataset on which the torch hub models were trained. This will be used to get the category label names from the predicted class ids."
      ],
      "id": "dense-cincinnati"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "suffering-update"
      },
      "source": [
        "json_url = \"https://dl.fbaipublicfiles.com/pyslowfast/dataset/class_names/kinetics_classnames.json\"\n",
        "json_filename = \"kinetics_classnames.json\"\n",
        "try: urllib.URLopener().retrieve(json_url, json_filename)\n",
        "except: urllib.request.urlretrieve(json_url, json_filename)"
      ],
      "id": "suffering-update",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "provincial-giant"
      },
      "source": [
        "with open(json_filename, \"r\") as f:\n",
        "    kinetics_classnames = json.load(f)\n",
        "\n",
        "# Create an id to label name mapping\n",
        "kinetics_id_to_classname = {}\n",
        "for k, v in kinetics_classnames.items():\n",
        "    kinetics_id_to_classname[v] = str(k).replace('\"', \"\")"
      ],
      "id": "provincial-giant",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nutritional-analyst"
      },
      "source": [
        "#### Define input transform"
      ],
      "id": "nutritional-analyst"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "defensive-integrity"
      },
      "source": [
        "side_size = 256\n",
        "mean = [0.45, 0.45, 0.45]\n",
        "std = [0.225, 0.225, 0.225]\n",
        "crop_size = 256\n",
        "num_frames = 8\n",
        "sampling_rate = 8\n",
        "frames_per_second = 30\n",
        "\n",
        "# Note that this transform is specific to the slow_R50 model.\n",
        "transform =  ApplyTransformToKey(\n",
        "    key=\"video\",\n",
        "    transform=Compose(\n",
        "        [\n",
        "            UniformTemporalSubsample(num_frames),\n",
        "            Lambda(lambda x: x/255.0),\n",
        "            NormalizeVideo(mean, std),\n",
        "            ShortSideScale(\n",
        "                size=side_size\n",
        "            ),\n",
        "            CenterCropVideo(crop_size=(crop_size, crop_size))\n",
        "        ]\n",
        "    ),\n",
        ")\n",
        "\n",
        "# The duration of the input clip is also specific to the model.\n",
        "clip_duration = (num_frames * sampling_rate)/frames_per_second"
      ],
      "id": "defensive-integrity",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "statistical-submission"
      },
      "source": [
        "#### Run Inference\n",
        "\n",
        "Download an example video."
      ],
      "id": "statistical-submission"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cutting-liability"
      },
      "source": [
        "url_link = \"https://dl.fbaipublicfiles.com/pytorchvideo/projects/archery.mp4\"\n",
        "video_path = 'archery.mp4'\n",
        "try: urllib.URLopener().retrieve(url_link, video_path)\n",
        "except: urllib.request.urlretrieve(url_link, video_path)"
      ],
      "id": "cutting-liability",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "applied-nickel"
      },
      "source": [
        "Load the video and transform it to the input format required by the model."
      ],
      "id": "applied-nickel"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "healthy-turtle"
      },
      "source": [
        "# Select the duration of the clip to load by specifying the start and end duration\n",
        "# The start_sec should correspond to where the action occurs in the video\n",
        "start_sec = 0\n",
        "end_sec = start_sec + clip_duration\n",
        "\n",
        "# Initialize an EncodedVideo helper class and load the video\n",
        "video = EncodedVideo.from_path(video_path)\n",
        "\n",
        "# Load the desired clip\n",
        "video_data = video.get_clip(start_sec=start_sec, end_sec=end_sec)\n",
        "\n",
        "# Apply a transform to normalize the video input\n",
        "video_data = transform(video_data)\n",
        "\n",
        "# Move the inputs to the desired device\n",
        "inputs = video_data[\"video\"]\n",
        "inputs = inputs.to(device)"
      ],
      "id": "healthy-turtle",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "emotional-castle"
      },
      "source": [
        "#### Get Predictions"
      ],
      "id": "emotional-castle"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "inner-controversy",
        "outputId": "410e5150-48aa-41e6-85fa-50cc7c7e6743",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Pass the input clip through the model\n",
        "preds = model(inputs[None, ...])\n",
        "\n",
        "# Get the predicted classes\n",
        "post_act = torch.nn.Softmax(dim=1)\n",
        "preds = post_act(preds)\n",
        "pred_classes = preds.topk(k=5).indices[0]\n",
        "\n",
        "# Map the predicted classes to the label names\n",
        "pred_class_names = [kinetics_id_to_classname[int(i)] for i in pred_classes]\n",
        "print(\"Top 5 predicted labels: %s\" % \", \".join(pred_class_names))"
      ],
      "id": "inner-controversy",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5 predicted labels: archery, throwing axe, playing paintball, stretching arm, riding or walking with horse\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "apart-somalia"
      },
      "source": [
        "### Model Description\n",
        "The model architecture is based on [1] with pretrained weights using the 8x8 setting\n",
        "on the Kinetics dataset. \n",
        "| arch | depth | frame length x sample rate | top 1 | top 5 | Flops (G) | Params (M) |\n",
        "| --------------- | ----------- | ----------- | ----------- | ----------- | ----------- |  ----------- | ----------- |\n",
        "| Slow     | R50   | 8x8                        | 74.58 | 91.63 | 54.52     | 32.45     |\n",
        "\n",
        "\n",
        "### References\n",
        "[1] Christoph Feichtenhofer et al, \"SlowFast Networks for Video Recognition\"\n",
        "https://arxiv.org/pdf/1812.03982.pdf"
      ],
      "id": "apart-somalia"
    }
  ]
}