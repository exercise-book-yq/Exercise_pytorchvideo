{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "X3D.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMRS7gVNp9cuvKp3xY/24S4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zhengyouqiang/Exercise_pytorchvideo/blob/main/X3D.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YXLATlodmddq",
        "outputId": "3184466c-0b55-4cda-c0e7-299c9fecd714"
      },
      "source": [
        "!pip install -q gradio pytorchvideo"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 2.0 MB 20.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 128 kB 50.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 206 kB 49.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.9 MB 39.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 49 kB 5.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 37.2 MB 32 kB/s \n",
            "\u001b[K     |████████████████████████████████| 596 kB 42.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.5 MB 37.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 63 kB 2.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 961 kB 41.8 MB/s \n",
            "\u001b[?25h  Building wheel for pytorchvideo (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for flask-cachebuster (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for fvcore (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQEoKJPimitg"
      },
      "source": [
        "import torch\n",
        "# Choose the `x3d_s` model \n",
        "import json\n",
        "import urllib\n",
        "import time\n",
        "from pytorchvideo.data.encoded_video import EncodedVideo\n",
        "\n",
        "from torchvision.transforms import Compose, Lambda\n",
        "from torchvision.transforms._transforms_video import (\n",
        "    CenterCropVideo,\n",
        "    NormalizeVideo,\n",
        ")\n",
        "from pytorchvideo.transforms import (\n",
        "    ApplyTransformToKey,\n",
        "    ShortSideScale,\n",
        "    UniformTemporalSubsample\n",
        ")\n",
        "import torch\n",
        "# Choose the `slow_r50` model\n",
        "from pytorchvideo.models.hub import (  # noqa: F401, E402\n",
        "    c2d_r50,\n",
        "    csn_r101,\n",
        "    efficient_x3d_s,\n",
        "    efficient_x3d_xs,\n",
        "    i3d_r50,\n",
        "    mvit_base_16,\n",
        "    mvit_base_16x4,\n",
        "    mvit_base_32x3,\n",
        "    r2plus1d_r50,\n",
        "    slow_r50,\n",
        "    slow_r50_detection,\n",
        "    slowfast_16x8_r101_50_50,\n",
        "    slowfast_r50,\n",
        "    slowfast_r50_detection,\n",
        "    slowfast_r101,\n",
        "    x3d_l,\n",
        "    x3d_m,\n",
        "    x3d_s,\n",
        "    x3d_xs,\n",
        ") \n",
        "model=x3d_s(pretrained=True)\n",
        "\n",
        "import gradio as gr\n",
        "\n",
        "#Video\n",
        "torch.hub.download_url_to_file('https://dl.fbaipublicfiles.com/pytorchvideo/projects/archery.mp4', 'archery.mp4')\n",
        "\n",
        "\n",
        "\n",
        "# Set to GPU or CPU\n",
        "device = \"cuda\"\n",
        "model = model.eval()\n",
        "model = model.to(device)\n",
        "\n",
        "json_url = \"https://dl.fbaipublicfiles.com/pyslowfast/dataset/class_names/kinetics_classnames.json\"\n",
        "json_filename = \"kinetics_classnames.json\"\n",
        "try: urllib.URLopener().retrieve(json_url, json_filename)\n",
        "except: urllib.request.urlretrieve(json_url, json_filename)\n",
        "\n",
        "with open(json_filename, \"r\") as f:\n",
        "    kinetics_classnames = json.load(f)\n",
        "\n",
        "# Create an id to label name mapping\n",
        "kinetics_id_to_classname = {}\n",
        "for k, v in kinetics_classnames.items():\n",
        "    kinetics_id_to_classname[v] = str(k).replace('\"', \"\")\n",
        "\n",
        "mean = [0.45, 0.45, 0.45]\n",
        "std = [0.225, 0.225, 0.225]\n",
        "frames_per_second = 30\n",
        "model_transform_params  = {\n",
        "    \"x3d_xs\": {\n",
        "        \"side_size\": 182,\n",
        "        \"crop_size\": 182,\n",
        "        \"num_frames\": 4,\n",
        "        \"sampling_rate\": 12,\n",
        "    },\n",
        "    \"x3d_s\": {\n",
        "        \"side_size\": 182,\n",
        "        \"crop_size\": 182,\n",
        "        \"num_frames\": 13,\n",
        "        \"sampling_rate\": 6,\n",
        "    },\n",
        "    \"x3d_m\": {\n",
        "        \"side_size\": 256,\n",
        "        \"crop_size\": 256,\n",
        "        \"num_frames\": 16,\n",
        "        \"sampling_rate\": 5,\n",
        "    }\n",
        "}\n",
        "\n",
        "# Get transform parameters based on model\n",
        "transform_params = model_transform_params[model_name]\n",
        "\n",
        "# Note that this transform is specific to the slow_R50 model.\n",
        "transform =  ApplyTransformToKey(\n",
        "    key=\"video\",\n",
        "    transform=Compose(\n",
        "        [\n",
        "            UniformTemporalSubsample(transform_params[\"num_frames\"]),\n",
        "            Lambda(lambda x: x/255.0),\n",
        "            NormalizeVideo(mean, std),\n",
        "            ShortSideScale(size=transform_params[\"side_size\"]),\n",
        "            CenterCropVideo(\n",
        "                crop_size=(transform_params[\"crop_size\"], transform_params[\"crop_size\"])\n",
        "            )\n",
        "        ]\n",
        "    ),\n",
        ")\n",
        "\n",
        "# The duration of the input clip is also specific to the model.\n",
        "clip_duration = (transform_params[\"num_frames\"] * transform_params[\"sampling_rate\"])/frames_per_second\n",
        "\n",
        "def x3dpred(video):\n",
        "\n",
        "  # Select the duration of the clip to load by specifying the start and end duration\n",
        "  # The start_sec should correspond to where the action occurs in the video\n",
        "  start_sec = 0\n",
        "  end_sec = start_sec + clip_duration\n",
        "\n",
        "  # Initialize an EncodedVideo helper class and load the video\n",
        "  video = EncodedVideo.from_path(video)\n",
        "\n",
        "  # Load the desired clip\n",
        "  video_data = video.get_clip(start_sec=start_sec, end_sec=end_sec)\n",
        "\n",
        "  # Apply a transform to normalize the video input\n",
        "  video_data = transform(video_data)\n",
        "\n",
        "  # Move the inputs to the desired device\n",
        "  inputs = video_data[\"video\"]\n",
        "  inputs = inputs.to(device)\n",
        "\n",
        "\n",
        "  # Pass the input clip through the model\n",
        "  preds = model(inputs[None, ...])\n",
        "\n",
        "  # calculate the time of prediction\n",
        "  start=time.time()\n",
        "  # Get the predicted classes\n",
        "  post_act = torch.nn.Softmax(dim=1)\n",
        "  preds = post_act(preds)\n",
        "  pred_classes = preds.topk(k=5).indices[0]\n",
        "  end=time.time()\n",
        "  print(end-start)\n",
        "  # Map the predicted classes to the label names\n",
        "  pred_class_names = [kinetics_id_to_classname[int(i)] for i in pred_classes]\n",
        "  return \"%s\" % \", \".join(pred_class_names)\n",
        "  \n",
        "inputs = gr.inputs.Video(label=\"Input Video\")\n",
        "outputs = gr.outputs.Textbox(label=\"Top 5 predicted labels\")\n",
        "\n",
        "title = \"X3D\"\n",
        "description = \"Gradio demo for X3D networks pretrained on the Kinetics 400 dataset. To use it, simply upload your video, or click one of the examples to load them. Read more at the links below.\"\n",
        "article = \"<p style='text-align: center'><a href='https://arxiv.org/abs/2004.04730'>X3D: Expanding Architectures for Efficient Video Recognition</a> | <a href='https://github.com/facebookresearch/pytorchvideo'>Github Repo</a></p>\"\n",
        "\n",
        "examples = [\n",
        "    ['archery.mp4']\n",
        "]\n",
        "\n",
        "gr.Interface(x3dpred, inputs, outputs, title=title, description=description, article=article, examples=examples, analytics_enabled=False).launch(debug=True)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}